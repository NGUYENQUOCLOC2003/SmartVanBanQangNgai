import os
from dotenv import load_dotenv # type: ignore
import streamlit as st # type: ignore

from langchain_community.vectorstores import FAISS # type: ignore
from langchain.memory import ConversationBufferMemory # type: ignore
from langchain.chains import ConversationalRetrievalChain # type: ignore
from langchain_google_genai import ChatGoogleGenerativeAI # type: ignore
from langchain_huggingface import HuggingFaceEmbeddings # type: ignore

load_dotenv()
working_dir = os.path.dirname(os.path.abspath(__file__))
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

VECTOR_DIR = os.path.join(working_dir, "vectorstore_data")

st.set_page_config(page_title="ğŸ¤– Chatbot SmartVÄƒnBáº£nQN - Trá»£ lÃ½ VÄƒn báº£n Quáº£ng NgÃ£i ThÃ´ng Minh", layout="wide")
st.title("ğŸ¤– ChatbotSmartVÄƒnBáº£nQN - Trá»£ lÃ½ VÄƒn báº£n Quáº£ng NgÃ£i ThÃ´ng Minh")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if os.path.exists(VECTOR_DIR):
    with st.spinner("ğŸš€ Äang táº£i dá»¯ liá»‡u Ä‘Ã£ há»c vui lÃ²ng chá» vÃ i giÃ¢y..."):
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-MiniLM-L6-v2")
        vectorstore = FAISS.load_local(
            folder_path=VECTOR_DIR,
            embeddings=embeddings,
            allow_dangerous_deserialization=True
        )

        llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        memory = ConversationBufferMemory(llm=llm, output_key="answer", memory_key="chat_history", return_messages=True)
        chain = ConversationalRetrievalChain.from_llm(
            llm=llm,
            retriever=retriever,
            chain_type="stuff",
            memory=memory,
            return_source_documents=True
        )

        st.session_state.conversation_chain = chain
else:
    st.error("âŒ ChÆ°a cÃ³ dá»¯ liá»‡u há»c. Vui lÃ²ng liÃªn há»‡ quáº£n trá»‹ viÃªn Ä‘á»ƒ cáº­p nháº­t tÃ i liá»‡u.")

# Hiá»ƒn thá»‹ lá»‹ch sá»­ chat
for message in st.session_state.chat_history:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Input
user_input = st.chat_input("ğŸ§® Há»i gÃ¬ vá» vÄƒn báº£n Quáº£ng NgÃ£i nÃ o?")

if user_input:
    st.chat_message("user").markdown(user_input)
    st.session_state.chat_history.append({"role": "user", "content": user_input})

    with st.chat_message("assistant"):
        with st.spinner("ğŸ’¬ Äang truy xuáº¥t vÃ  suy luáº­n..."):
            try:
                if "conversation_chain" not in st.session_state:
                    raise Exception("âŒ Dá»¯ liá»‡u chÆ°a sáºµn sÃ ng.")

                response = st.session_state.conversation_chain.invoke({"question": user_input})
                source_docs = response.get("source_documents", [])

                if not source_docs:
                    assistant_response = "âŒ CÃ¢u há»i nÃ y khÃ´ng cÃ³ trong tÃ i liá»‡u Ä‘Ã£ há»c. Vui lÃ²ng há»i láº¡i theo ná»™i dung ToÃ¡n 12."
                else:
                    assistant_response = response.get("answer", "âŒ KhÃ´ng cÃ³ cÃ¢u tráº£ lá»i phÃ¹ há»£p.")
            except Exception as e:
                assistant_response = str(e)

            st.markdown(assistant_response)
            st.session_state.chat_history.append({"role": "assistant", "content": assistant_response})
