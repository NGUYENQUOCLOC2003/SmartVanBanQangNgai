import os
import json
import shutil
from dotenv import load_dotenv # type: ignore
import streamlit as st # type: ignore

from langchain_community.vectorstores import FAISS # type: ignore
from langchain.memory import ConversationBufferMemory # type: ignore
from langchain.chains import ConversationalRetrievalChain # type: ignore
from langchain_google_genai import ChatGoogleGenerativeAI # type: ignore
from langchain_huggingface import HuggingFaceEmbeddings # type: ignore
from langchain_text_splitters.character import RecursiveCharacterTextSplitter # type: ignore
from langchain_community.document_loaders import ( # type: ignore
    PyPDFLoader, Docx2txtLoader,
    UnstructuredExcelLoader, UnstructuredPowerPointLoader
)

load_dotenv()
working_dir = os.path.dirname(os.path.abspath(__file__))
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

VECTOR_DIR = os.path.join(working_dir, "vectorstore_data")
UPLOAD_DIR = os.path.join(working_dir, "upload")
LEARNED_FILE = os.path.join(working_dir, "learned_files.json")

st.set_page_config(page_title="üìÅ Qu·∫£n tr·ªã SmartVƒÉnB·∫£nQN", layout="wide")
st.title("üìÅ Qu·∫£n l√Ω t√†i li·ªáu h·ªçc cho Chatbot SmartVƒÉnB·∫£nQN - Tr·ª£ l√Ω VƒÉn b·∫£n Qu·∫£ng Ng√£i Th√¥ng Minh")

# Cho ph√©p upload tr·ª±c ti·∫øp
uploaded_files = st.file_uploader(
    label="üìÇ T·∫£i t√†i li·ªáu SmartVƒÉnB·∫£nQN (PDF, Word, Excel...)",
    type=["pdf", "doc", "docx", "xls", "xlsx", "ppt", "pptx"],
    accept_multiple_files=True
)

if uploaded_files:
    st.info(f"üì• ƒêang l∆∞u {len(uploaded_files)} t√†i li·ªáu v√†o th∆∞ m·ª•c upload/")
    os.makedirs(UPLOAD_DIR, exist_ok=True)
    for uploaded_file in uploaded_files:
        file_path = os.path.join(UPLOAD_DIR, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
    st.success("‚úÖ ƒê√£ l∆∞u file v√†o th∆∞ m·ª•c upload. F5 ƒë·ªÉ h·ªá th·ªëng h·ªçc th√™m.")

def load_learned_files():
    if os.path.exists(LEARNED_FILE):
        with open(LEARNED_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_learned_files(file_list):
    with open(LEARNED_FILE, "w", encoding="utf-8") as f:
        json.dump(file_list, f, indent=2)

def load_document(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".pdf":
        loader = PyPDFLoader(file_path)
    elif ext in [".doc", ".docx"]:
        loader = Docx2txtLoader(file_path)
    elif ext in [".xls", ".xlsx"]:
        loader = UnstructuredExcelLoader(file_path)
    elif ext in [".ppt", ".pptx"]:
        loader = UnstructuredPowerPointLoader(file_path)
    else:
        raise ValueError(f"‚ùå ƒê·ªãnh d·∫°ng kh√¥ng h·ªó tr·ª£: {ext}")
    return loader.load()

def setup_vectorstore(documents):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-MiniLM-L6-v2")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
    doc_chunks = text_splitter.split_documents(documents)
    return FAISS.from_documents(doc_chunks, embeddings)

def create_chain(vectorstore):
    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    memory = ConversationBufferMemory(llm=llm, output_key="answer", memory_key="chat_history", return_messages=True)
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        memory=memory,
        verbose=False,
        return_source_documents=True
    )

learned_files = load_learned_files()
learned_index = {item["name"]: item["modified"] for item in learned_files}

new_files = []
if os.path.exists(UPLOAD_DIR):
    for filename in os.listdir(UPLOAD_DIR):
        file_path = os.path.join(UPLOAD_DIR, filename)
        modified_time = os.path.getmtime(file_path)
        if filename not in learned_index or learned_index[filename] != modified_time:
            new_files.append((filename, modified_time))
else:
    os.makedirs(UPLOAD_DIR)

if new_files:
    st.warning(f"üîç Ph√°t hi·ªán {len(new_files)} t√†i li·ªáu m·ªõi...")
    documents = []
    for filename, _ in new_files:
        file_path = os.path.join(UPLOAD_DIR, filename)
        try:
            docs = load_document(file_path)
            documents.extend(docs)
            st.write(f"üìÑ ƒê√£ ƒë·ªçc: {filename}")
        except Exception as e:
            st.error(f"L·ªói khi ƒë·ªçc {filename}: {str(e)}")

    if documents:
        with st.spinner("üì¶ ƒêang x·ª≠ l√Ω v√† c·∫≠p nh·∫≠t FAISS..."):
            embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-MiniLM-L6-v2")
            new_vectorstore = setup_vectorstore(documents)

            if os.path.exists(VECTOR_DIR):
                existing = FAISS.load_local(
                    folder_path=VECTOR_DIR,
                    embeddings=embeddings,
                    allow_dangerous_deserialization=True
                )
                existing.merge_from(new_vectorstore)
                existing.save_local(VECTOR_DIR)
            else:
                new_vectorstore.save_local(VECTOR_DIR)

            learned_files += [
                {"name": name, "modified": mod} for name, mod in new_files
            ]
            save_learned_files(learned_files)
        st.success("‚úÖ ƒê√£ th√™m d·ªØ li·ªáu m·ªõi v√†o b·ªô nh·ªõ FAISS!")
else:
    st.success("‚úÖ Kh√¥ng c√≥ t√†i li·ªáu m·ªõi c·∫ßn x·ª≠ l√Ω.")

if st.button("üóëÔ∏è X√≥a to√†n b·ªô d·ªØ li·ªáu ƒë√£ h·ªçc"):
    if os.path.exists(VECTOR_DIR):
        shutil.rmtree(VECTOR_DIR)
    if os.path.exists(LEARNED_FILE):
        os.remove(LEARNED_FILE)
    st.success("‚úÖ ƒê√£ x√≥a to√†n b·ªô d·ªØ li·ªáu h·ªçc. H√£y t·∫£i l·∫°i t√†i li·ªáu v√†o th∆∞ m·ª•c 'upload/' v√† F5 l·∫°i.")
